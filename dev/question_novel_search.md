

ã€é—®é¢˜ã€‘å¦‚ä½•æœç´¢ å°è¯´é‡Œçš„äººç‰©å‡ºç°çš„æ‰€æœ‰ç‰‡æ®µ ã€2ã€å¦‚ä½•æœç´¢æŸä¸ªäº‹ä»¶ 3ã€äººç‰©å…³ç³»

# ğŸ“š å°è¯´æ™ºèƒ½åˆ†æå®æˆ˜ï¼šäººç‰©ç‰‡æ®µæœç´¢ + äº‹ä»¶è¿½è¸ª + å…³ç³»å›¾è°±  
*ï¼ˆåŸºäº Elasticsearch 8.11 + NLPå¢å¼º Â· é€‚é…ä¸­æ–‡å°è¯´åœºæ™¯ï¼‰*

---

## ğŸŒŸ æ ¸å¿ƒæ€è·¯ï¼šä¸‰å±‚èƒ½åŠ›æ¶æ„
```mermaid
flowchart TD
    A[åŸå§‹å°è¯´æ–‡æœ¬] --> B[æ™ºèƒ½ç´¢å¼•å±‚]
    B --> C[æœç´¢å±‚]
    B --> D[å…³ç³»å±‚]
    
    subgraph B [æ™ºèƒ½ç´¢å¼•å±‚]
        B1[æ–‡æœ¬åˆ†å—] --> B2[NLPå®ä½“è¯†åˆ«]
        B2 --> B3[äººç‰©/äº‹ä»¶æ ‡ç­¾]
        B3 --> B4[ESç´¢å¼•]
    end
    
    subgraph C [æœç´¢å±‚]
        C1[äººç‰©ç‰‡æ®µæœç´¢] --> C2[äº‹ä»¶è¯­ä¹‰æœç´¢]
    end
    
    subgraph D [å…³ç³»å±‚]
        D1[å…±ç°åˆ†æ] --> D2[å…³ç³»å¼ºåº¦è®¡ç®—]
        D2 --> D3[å›¾è°±å¯è§†åŒ–]
    end
```

> ğŸ’¡ **å…³é”®å‰æ**ï¼šæ™®é€šå…¨æ–‡æœç´¢æ— æ³•ç›´æ¥ç†è§£"äººç‰©/äº‹ä»¶"ï¼Œéœ€é€šè¿‡ **NLPé¢„å¤„ç† + ç»“æ„åŒ–æ ‡ç­¾** å¢å¼ºç´¢å¼•

---

## ğŸ” ç¬¬ä¸€éƒ¨åˆ†ï¼šäººç‰©ç‰‡æ®µæœç´¢ï¼ˆç²¾å‡†å®šä½è§’è‰²å‡ºåœºï¼‰

### æ­¥éª¤1ï¼šç´¢å¼•å¢å¼ºï¼ˆæ·»åŠ äººç‰©æ ‡ç­¾å­—æ®µï¼‰
```python
# preprocess_novel.py - ä½¿ç”¨ spaCy ä¸­æ–‡æ¨¡å‹æå–äººç‰©
import spacy
from elasticsearch import Elasticsearch
from pathlib import Path

# åŠ è½½ä¸­æ–‡NLPæ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€ä¸‹è½½ï¼špython -m spacy download zh_core_web_smï¼‰
nlp = spacy.load("zh_core_web_sm") 

# é¢„å®šä¹‰å°è¯´äººç‰©è¯å…¸ï¼ˆé¿å…NLPæ¼è¯†åˆ«ï¼‰
CHARACTER_DICT = ["æ—é»›ç‰", "è´¾å®ç‰", "è–›å®é’—", "ç‹ç†™å‡¤", "è¯¸è‘›äº®", "æ›¹æ“"] 

def extract_characters(text):
    """æå–æ–‡æœ¬ä¸­å‡ºç°çš„äººç‰©ï¼ˆç»“åˆè¯å…¸+NERï¼‰"""
    # æ–¹æ³•1ï¼šåŸºäºé¢„å®šä¹‰è¯å…¸åŒ¹é…
    found = [char for char in CHARACTER_DICT if char in text]
    
    # æ–¹æ³•2ï¼šNLPå®ä½“è¯†åˆ«ï¼ˆè¡¥å……è¯å…¸æœªè¦†ç›–äººç‰©ï¼‰
    doc = nlp(text)
    for ent in doc.ents:
        if ent.label_ == "PERSON" and ent.text not in found:
            found.append(ent.text)
    
    return list(set(found))  # å»é‡

# ç´¢å¼•å°è¯´æ®µè½ï¼ˆæŒ‰500å­—åˆ†å—ï¼‰
es = Elasticsearch("http://localhost:9200")
INDEX_NAME = "novel_segments"

if not es.indices.exists(index=INDEX_NAME):
    es.indices.create(index=INDEX_NAME, body={
        "settings": {
            "analysis": {
                "analyzer": {
                    "novel_analyzer": {
                        "type": "custom",
                        "tokenizer": "ik_max_word",
                        "filter": ["lowercase"]
                    }
                }
            }
        },
        "mappings": {
            "properties": {
                "content": {"type": "text", "analyzer": "novel_analyzer"},
                "chapter": {"type": "integer"},
                "paragraph_start": {"type": "integer"},  # æ®µè½èµ·å§‹ä½ç½®
                "characters": {  # å…³é”®ï¼šäººç‰©æ ‡ç­¾æ•°ç»„
                    "type": "keyword"
                },
                "events": {  # äº‹ä»¶æ ‡ç­¾ï¼ˆåç»­ç”¨ï¼‰
                    "type": "keyword"
                }
            }
        }
    })

# åˆ†å—ç´¢å¼•ç¤ºä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
novel_text = Path("dream_of_red_mansion.txt").read_text(encoding="utf-8")
chunk_size = 500

for i in range(0, len(novel_text), chunk_size):
    chunk = novel_text[i:i+chunk_size]
    chars = extract_characters(chunk)
    
    es.index(index=INDEX_NAME, document={
        "content": chunk,
        "chapter": i // 10000 + 1,  # ç®€åŒ–ç« èŠ‚è®¡ç®—
        "paragraph_start": i,
        "characters": chars,
        "events": []  # åç»­è¡¥å……
    })
```

### æ­¥éª¤2ï¼šæœç´¢äººç‰©æ‰€æœ‰å‡ºåœºç‰‡æ®µï¼ˆKibana Dev Toolsï¼‰
```json
// æ–¹æ¡ˆAï¼šç²¾å‡†äººç‰©æ ‡ç­¾æœç´¢ï¼ˆæ¨èï¼‰
GET novel_segments/_search
{
  "query": {
    "term": {
      "characters": "æ—é»›ç‰"  // æ³¨æ„ï¼škeywordç±»å‹ç”¨termï¼Œtextç±»å‹ç”¨match
    }
  },
  "highlight": {
    "fields": {
      "content": {
        "pre_tags": ["<mark style='background:yellow'>"],
        "post_tags": ["</mark>"],
        "fragment_size": 150,   // é«˜äº®ç‰‡æ®µé•¿åº¦
        "number_of_fragments": 3
      }
    }
  },
  "sort": [
    {"chapter": "asc"},
    {"paragraph_start": "asc"}
  ],
  "size": 20  // æ¯é¡µ20ä¸ªç‰‡æ®µ
}

// æ–¹æ¡ˆBï¼šå…¨æ–‡æ¨¡ç³Šæœç´¢ï¼ˆå…œåº•æ–¹æ¡ˆï¼Œå¯èƒ½è¯¯åŒ¹é…ï¼‰
GET novel_segments/_search
{
  "query": {
    "match_phrase": {  // çŸ­è¯­åŒ¹é…é¿å…"æ—"å’Œ"é»›ç‰"åˆ†å¼€åŒ¹é…
      "content": "æ—é»›ç‰"
    }
  }
}
```

### âœ… æœç´¢ç»“æœè§£è¯»ï¼š
```json
{
  "hits": {
    "total": { "value": 142, "relation": "eq" },  // æ—é»›ç‰å…±å‡ºåœº142æ¬¡
    "hits": [
      {
        "_source": {
          "chapter": 3,
          "paragraph_start": 12450,
          "content": "...åªè§é‚£è¾¹æ¥äº†ä¸€ä¸ªè¢…è¢…å©·å©·çš„å¥³å„¿ï¼Œä¾¿æ–™å®šæ˜¯æ—é»›ç‰..."
        },
        "highlight": {
          "content": [
            "...æ–™å®šæ˜¯<mark>æ—é»›ç‰</mark>..."
          ]
        }
      }
    ]
  }
}
```

> ğŸ’¡ **æŠ€å·§**ï¼šåœ¨Kibanaä¸­ç‚¹å‡»"â†‘"æŒ‰é’®å¯å¯¼å‡ºæ‰€æœ‰ç»“æœä¸ºCSVï¼Œç”¨äºåç»­åˆ†æ

---

## ğŸ”¥ ç¬¬äºŒéƒ¨åˆ†ï¼šäº‹ä»¶æœç´¢ï¼ˆè¯­ä¹‰çº§äº‹ä»¶è¿½è¸ªï¼‰

### äº‹ä»¶å®šä¹‰ç­–ç•¥ï¼ˆä¸‰ç§æ–¹æ¡ˆï¼‰
| æ–¹æ¡ˆ | é€‚ç”¨åœºæ™¯ | å®ç°æ–¹å¼ |
|------|----------|----------|
| **å…³é”®è¯ç»„åˆ** | ç®€å•äº‹ä»¶ï¼ˆå¦‚"è‘¬èŠ±"ï¼‰ | `match_phrase` + åŒä¹‰è¯æ‰©å±• |
| **äº‹ä»¶æ¨¡æ¿** | å¤æ‚äº‹ä»¶ï¼ˆå¦‚"äººç‰©Aå¯¹äººç‰©Bè¡¨ç™½"ï¼‰ | è‡ªå®šä¹‰æŸ¥è¯¢æ¨¡æ¿ + è„šæœ¬è¯„åˆ† |
| **NLPäº‹ä»¶æŠ½å–** | é«˜çº§éœ€æ±‚ï¼ˆéœ€è®­ç»ƒæ¨¡å‹ï¼‰ | spaCyè§„åˆ™/Transformersæ¨¡å‹é¢„å¤„ç† |

### å®æˆ˜ï¼šæœç´¢"é»›ç‰è‘¬èŠ±"äº‹ä»¶
```json
// æ–¹æ¡ˆ1ï¼šå…³é”®è¯ç»„åˆï¼ˆåŸºç¡€ï¼‰
GET novel_segments/_search
{
  "query": {
    "bool": {
      "must": [
        { "match_phrase": { "content": "é»›ç‰" } },
        { "match_phrase": { "content": "è‘¬èŠ±" } }
      ],
      "filter": [
        { "range": { "chapter": { "gte": 20, "lte": 30 } } }  // é™å®šç« èŠ‚èŒƒå›´ï¼ˆå·²çŸ¥è‘¬èŠ±åœ¨27å›ï¼‰
      ]
    }
  }
}

// æ–¹æ¡ˆ2ï¼šåŒä¹‰è¯æ‰©å±•ï¼ˆæ›´é²æ£’ï¼‰
GET novel_segments/_search
{
  "query": {
    "match": {
      "content": {
        "query": "é»›ç‰ è‘¬èŠ± èŠ±å†¢ é”¦å›Š",
        "operator": "or",
        "fuzziness": "AUTO"  // å…è®¸é”™åˆ«å­—
      }
    }
  }
}

// æ–¹æ¡ˆ3ï¼šäº‹ä»¶æ¨¡æ¿ï¼ˆé«˜çº§ - æœç´¢"äººç‰©æ­»äº¡"äº‹ä»¶ï¼‰
GET novel_segments/_search
{
  "query": {
    "script_score": {
      "query": {
        "match": { "content": "æ­»äº† é€ä¸– æ® æ•…å»" }
      },
      "script": {
        "source": """
          // è®¡ç®—äººç‰©åä¸æ­»äº¡è¯çš„è·ç¦»ï¼ˆè¶Šè¿‘è¶Šç›¸å…³ï¼‰
          def content = params._source.content;
          def death_words = ['æ­»äº†','é€ä¸–','æ®'];
          def chars = params._source.characters;
          
          if (chars.length == 0) return 0.1;
          
          // ç®€å•è·ç¦»è®¡ç®—ï¼šäººç‰©ååœ¨æ­»äº¡è¯å‰100å­—ç¬¦å†…
          for (char in chars) {
            if (content.indexOf(char) > -1 && 
                content.indexOf(char) < content.indexOf(death_words[0]) + 100) {
              return 2.0;
            }
          }
          return 0.5;
        """
      }
    }
  }
}
```

### âš ï¸ äº‹ä»¶æœç´¢éš¾ç‚¹ä¸å¯¹ç­–ï¼š
| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| äº‹ä»¶æè¿°åˆ†æ•£ï¼ˆå¦‚"è‘¬èŠ±"åˆ†3æ®µæå†™ï¼‰ | ç”¨`"slop": 50`å…è®¸çŸ­è¯­è¯åºçµæ´»ï¼š`"match_phrase": { "content": { "query": "é»›ç‰ è‘¬èŠ±", "slop": 50 } }` |
| åŒä¸€äº‹ä»¶å¤šç§è¯´æ³•ï¼ˆ"è‘¬èŠ±"/"åŸ‹é¦™å†¢"ï¼‰ | åˆ›å»ºåŒä¹‰è¯è¯å…¸ï¼š`è‘¬èŠ±, åŸ‹é¦™å†¢, è‘¬èŠ±åŸ => è‘¬èŠ±äº‹ä»¶` |
| äº‹ä»¶è·¨æ®µè½ | ç´¢å¼•æ—¶åˆå¹¶ç›¸é‚»æ®µè½ï¼ˆéœ€ä¸šåŠ¡é€»è¾‘åˆ¤æ–­ï¼‰ |

---

## ğŸ‘¥ ç¬¬ä¸‰éƒ¨åˆ†ï¼šäººç‰©å…³ç³»åˆ†æï¼ˆå…±ç°ç½‘ç»œï¼‰

### æ­¥éª¤1ï¼šæå–äººç‰©å…±ç°æ•°æ®
```python
# relationship_analyzer.py
from elasticsearch import Elasticsearch
from collections import defaultdict
import networkx as nx  # ç”¨äºå›¾è°±è®¡ç®—

es = Elasticsearch("http://localhost:9200")

# è·å–æ‰€æœ‰å«äººç‰©çš„æ®µè½
query = {
  "query": {
    "bool": {
      "must": { "exists": { "field": "characters" } },
      "filter": { "script": { "script": "doc['characters'].size() > 1" } }  # è‡³å°‘2äººåŒç°
    }
  },
  "size": 10000,
  "_source": ["characters", "chapter"]
}

resp = es.search(index="novel_segments", body=query)
co_occurrence = defaultdict(int)
chapter_cooccurrence = defaultdict(lambda: defaultdict(int))

for hit in resp['hits']['hits']:
    chars = hit['_source']['characters']
    chapter = hit['_source']['chapter']
    
    # æ— åºå¯¹ï¼šé¿å…(A,B)å’Œ(B,A)é‡å¤è®¡æ•°
    for i in range(len(chars)):
        for j in range(i+1, len(chars)):
            pair = tuple(sorted([chars[i], chars[j]]))
            co_occurrence[pair] += 1
            chapter_cooccurrence[chapter][pair] += 1

# è¾“å‡ºTop10å…³ç³»
top_relations = sorted(co_occurrence.items(), key=lambda x: x[1], reverse=True)[:10]
print("äººç‰©å…³ç³»å¼ºåº¦ï¼ˆå…±ç°æ¬¡æ•°ï¼‰:")
for (char1, char2), count in top_relations:
    print(f"{char1} â†” {char2}: {count}æ¬¡")
```

### æ­¥éª¤2ï¼šæ„å»ºå…³ç³»å›¾è°±ï¼ˆPython + PyVisï¼‰
```python
# visualize_relations.py
from pyvis.network import Network

# åˆ›å»ºå›¾è°±
net = Network(height="750px", width="100%", bgcolor="#222222", font_color="white")
net.barnes_hut()  # ä¼˜åŒ–å¸ƒå±€

# æ·»åŠ èŠ‚ç‚¹ï¼ˆäººç‰©ï¼‰
all_chars = set()
for (c1, c2), _ in top_relations:
    all_chars.update([c1, c2])

for char in all_chars:
    net.add_node(char, label=char, size=25, title=f"{char}å‡ºåœºæ¬¡æ•°å¾…ç»Ÿè®¡")

# æ·»åŠ è¾¹ï¼ˆå…³ç³»ï¼‰
for (char1, char2), weight in top_relations:
    net.add_edge(char1, char2, value=weight, title=f"å…±ç°{weight}æ¬¡")

net.show("character_relations.html")  # ç”Ÿæˆäº¤äº’å¼HTML
```

### æ­¥éª¤3ï¼šé«˜çº§å…³ç³»åˆ†æï¼ˆElasticsearchèšåˆï¼‰
```json
// æŒ‰ç« èŠ‚åˆ†æäººç‰©å…³ç³»æ¼”å˜
GET novel_segments/_search
{
  "size": 0,
  "aggs": {
    "by_chapter": {
      "terms": { "field": "chapter", "size": 120 },
      "aggs": {
        "character_pairs": {
          "terms": { 
            "script": """
              // ç”Ÿæˆäººç‰©å¯¹ï¼ˆæ— åºï¼‰
              def chars = doc['characters'];
              if (chars.size() < 2) return null;
              def pairs = [];
              for (int i=0; i<chars.size(); i++) {
                for (int j=i+1; j<chars.size(); j++) {
                  pairs.add(chars[i] + '|' + chars[j]);
                }
              }
              return pairs;
            """,
            "size": 50
          }
        }
      }
    }
  }
}
```

### ğŸ“Š å…³ç³»åˆ†æå®æˆ˜æ¡ˆä¾‹ï¼ˆã€Šçº¢æ¥¼æ¢¦ã€‹ç‰‡æ®µï¼‰ï¼š
| å…³ç³»å¯¹ | å…±ç°æ¬¡æ•° | å…³é”®ç« èŠ‚ | å…³ç³»è§£è¯» |
|--------|----------|----------|----------|
| è´¾å®ç‰ â†” æ—é»›ç‰ | 287 | 3, 23, 27, 32 | æƒ…æ„Ÿä¸»çº¿ï¼Œè‘¬èŠ±ã€è¯»è¥¿å¢ç­‰å…³é”®äº‹ä»¶ |
| è´¾å®ç‰ â†” è–›å®é’— | 198 | 8, 28, 34 | é‡‘ç‰è‰¯ç¼˜ï¼Œå¤šæ¬¡å¯¹æ¯”æå†™ |
| ç‹ç†™å‡¤ â†” è´¾æ¯ | 156 | 6, 40, 42 | æƒåŠ›ä¾é™„ï¼Œè®¨å¥½ä¸æŒæ§ |
| æ—é»›ç‰ â†” è–›å®é’— | 89 | 42, 45 | ä»æ•Œå¯¹åˆ°å’Œè§£çš„è½¬å˜ |

> ğŸ’¡ **æ·±åº¦æ´å¯Ÿ**ï¼šç»“åˆç« èŠ‚èšåˆç»“æœï¼Œå¯å‘ç°ï¼š
> - ç¬¬27å›ï¼ˆè‘¬èŠ±ï¼‰åï¼Œå®ç‰-é»›ç‰å…±ç°å¼ºåº¦éª¤å¢30%
> - ç¬¬42å›åï¼Œé»›ç‰-å®é’—å…³ç³»ä»è´Ÿå‘è½¬ä¸ºæ­£å‘ï¼ˆæ–‡æœ¬æƒ…æ„Ÿåˆ†æå¯éªŒè¯ï¼‰

---

## ğŸš€ ä¸€é”®å¯åŠ¨ï¼šå®Œæ•´å·¥ä½œæµè„šæœ¬
```bash
#!/bin/bash
# novel_analysis.sh

echo "ğŸš€ å°è¯´æ™ºèƒ½åˆ†ææµæ°´çº¿å¯åŠ¨..."

# 1. ç´¢å¼•å°è¯´ï¼ˆè‡ªåŠ¨åˆ†å—+NLPæ ‡æ³¨ï¼‰
python preprocess_novel.py --file dream_of_red_mansion.txt --index novel_segments

# 2. ç”Ÿæˆäººç‰©å…³ç³»æ•°æ®
python relationship_analyzer.py --index novel_segments --output relations.json

# 3. å¯åŠ¨äº¤äº’å¼å›¾è°±
python visualize_relations.py relations.json

# 4. æ‰“å¼€Kibanaå‡†å¤‡æœç´¢
echo "âœ… å‡†å¤‡å°±ç»ªï¼æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š"
echo "   1. æ‰“å¼€ http://localhost:5601"
echo "   2. è¿›å…¥ Dev Tools"
echo "   3. ç²˜è´´äººç‰©æœç´¢æŸ¥è¯¢ï¼ˆè§ä¸Šæ–‡ï¼‰"
echo "   4. æ‰“å¼€ character_relations.html æŸ¥çœ‹å…³ç³»å›¾è°±"
```

---

## ğŸ“Œ å…³é”®æ³¨æ„äº‹é¡¹

### ä¸­æ–‡å°è¯´ç‰¹æ®ŠæŒ‘æˆ˜
| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |
|------|----------|
| **äººç‰©åˆ«ç§°**ï¼ˆé»›ç‰/é¢¦å„¿/æ—å§‘å¨˜ï¼‰ | å»ºç«‹åˆ«åå­—å…¸æ˜ å°„ï¼š`"é¢¦å„¿": "æ—é»›ç‰"`ï¼Œç´¢å¼•å‰ç»Ÿä¸€æ›¿æ¢ |
| **ä»£è¯æŒ‡ä»£**ï¼ˆ"ä»–"æŒ‡ä»£è°ï¼Ÿï¼‰ | éœ€è¦å…±æŒ‡æ¶ˆè§£ï¼ˆCoreference Resolutionï¼‰ï¼Œæ¨èç”¨ [LTP](https://ltp.ai/) æˆ– spaCy + è§„åˆ™ |
| **å¤æ–‡åˆ†è¯** | IKåˆ†è¯å™¨å¯¹å¤æ–‡æ”¯æŒå¼±ï¼Œéœ€è‡ªå®šä¹‰è¯å…¸æ·»åŠ "è‘¬èŠ±åŸ""å¤ªè™šå¹»å¢ƒ"ç­‰ä¸“æœ‰åè¯ |

### æ€§èƒ½ä¼˜åŒ–å»ºè®®
```python
# æ‰¹é‡ç´¢å¼•ï¼ˆ1000æ–‡æ¡£/æ‰¹ï¼‰æ¯”å•æ¡ç´¢å¼•å¿«50å€
actions = [
    {"_index": "novel_segments", "_source": doc} 
    for doc in documents
]
helpers.bulk(es, actions)
```

### å®‰å…¨ç”Ÿäº§ç¯å¢ƒ
- äººç‰©/äº‹ä»¶æ ‡ç­¾å»ºè®®ç”¨ **ç‹¬ç«‹ç´¢å¼•** å­˜å‚¨ï¼ˆ`novel_metadata`ï¼‰ï¼Œä¸æ–‡æœ¬ç´¢å¼•å…³è”
- æ•æ„Ÿå°è¯´å†…å®¹éœ€åŠ å¯†å­˜å‚¨ï¼ˆES 8.0+ æ”¯æŒå­—æ®µçº§åŠ å¯†ï¼‰

---

## ğŸ’¡ ç»ˆæå»ºè®®ï¼šä»å°å¤„ç€æ‰‹

```mermaid
flowchart LR
    A[é€‰1ä¸ªçŸ­ç¯‡å°è¯´] --> B[ç´¢å¼•100ä¸ªæ®µè½]
    B --> C[æœç´¢1ä¸ªäººç‰©]
    C --> D[å¯¼å‡º5ä¸ªç‰‡æ®µäººå·¥éªŒè¯]
    D --> E[æ‰©å±•åˆ°æ•´æœ¬å°è¯´]
    E --> F[æ·»åŠ äº‹ä»¶/å…³ç³»åˆ†æ]
```

> âœ¨ **ä»Šæ—¥å¯å®Œæˆ**ï¼š  
> 1ï¸âƒ£ ç”¨500å­—ã€Šçº¢æ¥¼æ¢¦ã€‹ç‰‡æ®µæµ‹è¯•äººç‰©æœç´¢  
> 2ï¸âƒ£ åœ¨Kibanaçœ‹åˆ°"æ—é»›ç‰"é«˜äº®ç»“æœå³æˆåŠŸ  
> 3ï¸âƒ£ ç”¨`co_occurrence`è„šæœ¬è¾“å‡º2ä¸ªäººç‰©å…³ç³»  
>   
> **è®°ä½**ï¼šæ–‡å­¦åˆ†ææ²¡æœ‰100%å‡†ç¡®ï¼Œç›®æ ‡æ˜¯**è¾…åŠ©äººç±»å‘ç°çº¿ç´¢**ï¼Œè€Œéæ›¿ä»£è§£è¯» ğŸŒ¸

> é™„ï¼šæ¨èæµ‹è¯•æ•°æ®é›†  
> - ã€Šçº¢æ¥¼æ¢¦ã€‹å‰10å›ï¼ˆçº¦5ä¸‡å­—ï¼‰  
> - GitHubå¼€æºï¼š[chinese-poetry/chinese-poetry](https://github.com/chinese-poetry/chinese-poetry)ï¼ˆå«ç»“æ„åŒ–æ•°æ®ï¼‰