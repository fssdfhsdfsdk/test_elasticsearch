# ğŸ“š å°è¯´äººç‰©å…³ç³»æ™ºèƒ½æŠ½å–ï¼šä»éšæ™¦æ–‡æœ¬ä¸­æŒ–æ˜ã€Œäº‹ä»¶-å…³ç³»ã€ç½‘ç»œ  
*ï¼ˆæ— éœ€ç¡¬ç¼–ç  Â· æ”¯æŒéšæ™¦è¡¨è¾¾ Â· å¯è½åœ°å®æˆ˜æ–¹æ¡ˆï¼‰*

---

## ğŸŒŸ æ ¸å¿ƒæŒ‘æˆ˜ä¸ç ´å±€æ€è·¯

| ä¼ ç»Ÿæ–¹æ¡ˆç—›ç‚¹                                    | æœ¬æ–¹æ¡ˆåˆ›æ–°ç‚¹                                       |
| ----------------------------------------------- | -------------------------------------------------- |
| âŒ å…³é”®è¯åŒ¹é…ï¼ˆ"çˆ±"â†’çˆ±æƒ…ï¼‰æ¼æ‰"ä»–å‡è§†å¥¹èƒŒå½±è‰¯ä¹…" | âœ… **è¯­ä¹‰ç†è§£**ï¼šç”¨LLMè¯†åˆ«éšæ™¦æƒ…æ„Ÿ/åŠ¨ä½œ             |
| âŒ ç¡¬ç¼–ç å…³ç³»ç±»å‹ï¼ˆä»…æ”¯æŒ"çˆ¶å­/å¤«å¦»"ï¼‰           | âœ… **å¼€æ”¾åŸŸæŠ½å–**ï¼šè‡ªåŠ¨å‘ç°ä»»æ„å…³ç³»ï¼ˆå¦‚"æš—ä¸­ä¿æŠ¤"ï¼‰ |
| âŒ å¿½ç•¥ä»£è¯æŒ‡ä»£ï¼ˆ"ä»–é€’ç»™å¥¹è¯"ä¸çŸ¥æ˜¯è°ï¼‰          | âœ… **å…±æŒ‡æ¶ˆè§£**ï¼šå°†"ä»–/å¥¹"æ˜ å°„åˆ°å…·ä½“äººç‰©            |
| âŒ æ— æ³•å®šä½åŸæ–‡ç‰‡æ®µ                              | âœ… **ç‰‡æ®µé”šå®š**ï¼šè¿”å›ç²¾ç¡®ä¸Šä¸‹æ–‡+å…³ç³»ç½®ä¿¡åº¦          |

---

## ğŸ§© åˆ†å±‚å®æ–½æ–¹æ¡ˆï¼ˆæŒ‰èµ„æºæŠ•å…¥é€‰æ‹©ï¼‰

### æ–¹æ¡ˆAï¼šè½»é‡çº§ Â· è§„åˆ™+LLMæ··åˆï¼ˆæ¨èèµ·æ­¥ï¼‰
> é€‚åˆï¼šå•æœ¬å°è¯´/ä¸ªäººé¡¹ç›® Â· 10åˆ†é’Ÿå¿«é€ŸéªŒè¯

#### æ­¥éª¤1ï¼šäººç‰©å®ä½“è¯†åˆ«ï¼ˆè§£å†³"è°æ˜¯è°"ï¼‰
```python
# ç”¨ spaCy + ä¸­æ–‡æ¨¡å‹ è¯†åˆ«æ˜¾æ€§äººç‰©
import spacy
nlp = spacy.load("zh_core_web_lg")  # éœ€å…ˆå®‰è£…ï¼špython -m spacy download zh_core_web_lg

text = "æ—é»›ç‰æ‹­æ³ªé“ï¼š'å®ç‰ï¼Œä½ ä½•è‹¦å¦‚æ­¤ï¼Ÿ' å®ç‰é»˜ç„¶ã€‚"
doc = nlp(text)

characters = set()
for ent in doc.ents:
    if ent.label_ in ["PERSON", "PROPN"]:  # ä¸­æ–‡æ¨¡å‹å¯èƒ½éœ€è°ƒæ•´æ ‡ç­¾
        characters.add(ent.text)

print(characters)  # {'æ—é»›ç‰', 'å®ç‰'}
```

#### æ­¥éª¤2ï¼šå…±æŒ‡æ¶ˆè§£ï¼ˆè§£å†³"ä»–/å¥¹"æŒ‡ä»£ï¼‰
```python
# ç”¨ neuralcorefï¼ˆè‹±æ–‡ï¼‰æˆ– LTPï¼ˆä¸­æ–‡ï¼‰å¤„ç†ä»£è¯
from ltp import LTP
ltp = LTP()

def resolve_coreference(text, characters):
    seg, hidden = ltp.seg([text])
    coref = ltp.coref(hidden)  # è¿”å›å…±æŒ‡é“¾
    # ç®€åŒ–é€»è¾‘ï¼šå°†ä»£è¯æ˜ å°„åˆ°æœ€è¿‘æåŠçš„äººç‰©
    # å®é™…éœ€æ„å»ºå…±æŒ‡å›¾ï¼ˆæ­¤å¤„çœç•¥å¤æ‚å®ç°ï¼‰
    return text.replace("ä»–", "å®ç‰").replace("å¥¹", "æ—é»›ç‰")  # ç¤ºä¾‹
```

#### æ­¥éª¤3ï¼šLLMå…³ç³»æŠ½å–ï¼ˆæ ¸å¿ƒ Â· å¤„ç†éšæ™¦è¡¨è¾¾ï¼‰
```python
# ç”¨æœ¬åœ°LLMï¼ˆå¦‚Qwen/ChatGLM3ï¼‰æˆ–APIæŠ½å–å…³ç³»
from openai import OpenAI  # å…¼å®¹Ollama/æœ¬åœ°æ¨¡å‹

client = OpenAI(base_url="http://localhost:11434/v1", api_key="ollama")

def extract_relations(text, characters):
    prompt = f"""ä»ä»¥ä¸‹å°è¯´ç‰‡æ®µä¸­æå–äººç‰©å…³ç³»äº‹ä»¶ã€‚è¦æ±‚ï¼š
1. è¯†åˆ«æ‰€æœ‰äººç‰©ï¼š{', '.join(characters)}
2. æŠ½å–ä»»æ„ä¸¤äººä¹‹é—´çš„äº’åŠ¨ï¼ˆæ˜¾æ€§/éšæ€§å‡å¯ï¼‰
3. ç”¨JSONæ ¼å¼è¾“å‡ºï¼ŒåŒ…å«ï¼š
   - äººç‰©A
   - äººç‰©B
   - å…³ç³»ç±»å‹ï¼ˆè‡ªç”±æè¿°ï¼Œå¦‚"æš—ä¸­ä¿æŠ¤"ã€"è¨€è¯­è¯•æ¢"ï¼‰
   - åŸæ–‡è¯æ®ç‰‡æ®µï¼ˆ20å­—å†…ï¼‰
   - ç½®ä¿¡åº¦(0-1)

æ–‡æœ¬ï¼š{text}

è¾“å‡ºç¤ºä¾‹ï¼š
[{{"person_a": "æ—é»›ç‰", "person_b": "è´¾å®ç‰", "relation": "æƒ…æ„Ÿè¯•æ¢", "evidence": "ä½ ä½•è‹¦å¦‚æ­¤", "confidence": 0.85}}]
"""
    response = client.chat.completions.create(
        model="qwen:7b",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3  # é™ä½éšæœºæ€§ä¿è¯ç¨³å®šæ€§
    )
    return response.choices[0].message.content

# æµ‹è¯•
text = "æ—é»›ç‰æ‹­æ³ªé“ï¼š'å®ç‰ï¼Œä½ ä½•è‹¦å¦‚æ­¤ï¼Ÿ' å®ç‰é»˜ç„¶ï¼Œè½¬èº«ç¦»å»æ—¶è¢–ä¸­æ»‘è½ä¸€æ–¹æ—§å¸•ã€‚"
print(extract_relations(text, {"æ—é»›ç‰", "è´¾å®ç‰"}))
```

**è¾“å‡ºç¤ºä¾‹**ï¼š
```json
[
  {
    "person_a": "æ—é»›ç‰",
    "person_b": "è´¾å®ç‰",
    "relation": "æƒ…æ„Ÿè„†å¼±ä¸å›é¿",
    "evidence": "æ‹­æ³ªé“...é»˜ç„¶è½¬èº«",
    "confidence": 0.92
  },
  {
    "person_a": "è´¾å®ç‰",
    "person_b": "æ—é»›ç‰",
    "relation": "éšç§˜ç‰µæŒ‚",
    "evidence": "è¢–ä¸­æ»‘è½æ—§å¸•",
    "confidence": 0.87
  }
]
```

> ğŸ’¡ **å…³é”®æŠ€å·§**ï¼š  
> - ç”¨ **few-shotæç¤º** æä¾›2-3ä¸ªéšæ™¦å…³ç³»ç¤ºä¾‹ï¼ˆå¦‚"æ‘”ç‰=æƒ…æ„Ÿæ¿€çƒˆ"ï¼‰  
> - è¦æ±‚LLM **è¾“å‡ºç½®ä¿¡åº¦**ï¼Œè¿‡æ»¤ä½è´¨é‡ç»“æœ  
> - å¯¹é•¿æ–‡æœ¬**åˆ†æ®µå¤„ç†**ï¼ˆæ¯æ®µ300å­—ï¼‰ï¼Œé¿å…ä¸Šä¸‹æ–‡ä¸¢å¤±

---

### æ–¹æ¡ˆBï¼šç”Ÿäº§çº§ Â· ç«¯åˆ°ç«¯æµæ°´çº¿ï¼ˆæ¨èé•¿æœŸä½¿ç”¨ï¼‰
> é€‚åˆï¼šå¤šæœ¬å°è¯´/äº§å“åŒ– Â· è‡ªåŠ¨åŒ–å…³ç³»å›¾è°±

#### æ¶æ„è®¾è®¡
```mermaid
flowchart TD
    A[åŸå§‹å°è¯´æ–‡æœ¬] --> B(æ–‡æœ¬åˆ†æ®µ+äººç‰©è¯†åˆ«)
    B --> C{å…³ç³»æŠ½å–å¼•æ“}
    C -->|æ˜¾æ€§å…³ç³»| D[è§„åˆ™å¼•æ“<br>ï¼ˆæ­£åˆ™åŒ¹é…åŠ¨ä½œè¯ï¼‰]
    C -->|éšæ€§å…³ç³»| E[LLMè¯­ä¹‰ç†è§£<br>ï¼ˆQwen/ChatGLMï¼‰]
    D & E --> F[å…³ç³»èåˆå»é‡]
    F --> G[(Elasticsearch<br>å­˜å‚¨åŸæ–‡+å…³ç³»)]
    F --> H[(Neo4j<br>å­˜å‚¨äººç‰©å…³ç³»å›¾è°±)]
    G --> I[æœç´¢æ¥å£ï¼š<br>â€œæŸ¥æ‰¾é»›ç‰ä¸å®ç‰çš„å†²çªäº‹ä»¶â€]
    H --> J[å¯è§†åŒ–ï¼š<br>äººç‰©å…³ç³»ç½‘ç»œå›¾]
```

#### å…³é”®ç»„ä»¶å®ç°

##### 1. å…³ç³»æŠ½å–å¼•æ“ï¼ˆæ··åˆç­–ç•¥ï¼‰
```python
class RelationExtractor:
    def __init__(self):
        self.llm = self.load_llm()  # æœ¬åœ°7Bæ¨¡å‹
        self.action_verbs = self.load_verbs()  # åŠ¨ä½œè¯åº“ï¼ˆæ‰“/æ•‘/èµ ...ï¼‰
    
    def extract(self, segment, characters):
        relations = []
        
        # ç­–ç•¥1ï¼šè§„åˆ™åŒ¹é…ï¼ˆé«˜ç²¾åº¦æ˜¾æ€§å…³ç³»ï¼‰
        for a in characters:
            for b in characters:
                if a == b: continue
                for verb in self.action_verbs:
                    if f"{a}{verb}{b}" in segment or f"{b}{verb}{a}" in segment:
                        relations.append({
                            "type": "explicit",
                            "relation": f"{verb}ï¼ˆæ˜¾æ€§ï¼‰",
                            "confidence": 0.95
                        })
        
        # ç­–ç•¥2ï¼šLLMæŠ½å–ï¼ˆè¦†ç›–éšæ€§å…³ç³»ï¼‰
        if not relations or random.random() < 0.3:  # 30%ç‰‡æ®µèµ°LLMä¿å¬å›
            llm_relations = self.llm_extract(segment, characters)
            relations.extend(llm_relations)
        
        return self.deduplicate(relations)  # å»é‡åˆå¹¶
```

##### 2. Elasticsearch ç´¢å¼•è®¾è®¡ï¼ˆæ”¯æŒå…³ç³»æœç´¢ï¼‰
```json
PUT /novel_relations
{
  "mappings": {
    "properties": {
      "book_id": { "type": "keyword" },
      "person_a": { "type": "keyword" },
      "person_b": { "type": "keyword" },
      "relation_type": { 
        "type": "text",
        "analyzer": "ik_max_word",  // ä¸­æ–‡åˆ†è¯
        "fields": { "keyword": { "type": "keyword" } }
      },
      "evidence_text": { "type": "text" },
      "evidence_start": { "type": "integer" },  // åŸæ–‡ä½ç½®é”šç‚¹
      "confidence": { "type": "float" },
      "chapter": { "type": "integer" }
    }
  }
}
```

##### 3. æœç´¢ç¤ºä¾‹ï¼šæŸ¥æ‰¾"é»›ç‰ä¸å®ç‰çš„å†²çªäº‹ä»¶"
```json
GET /novel_relations/_search
{
  "query": {
    "bool": {
      "must": [
        { "terms": { "person_a.keyword": ["æ—é»›ç‰", "è´¾å®ç‰"] } },
        { "terms": { "person_b.keyword": ["æ—é»›ç‰", "è´¾å®ç‰"] } },
        { 
          "match": { 
            "relation_type": {
              "query": "å†²çª äº‰åµ äº‰æ‰§ åç›®",
              "operator": "or"
            }
          }
        }
      ],
      "filter": [
        { "range": { "confidence": { "gte": 0.7 } } }
      ]
    }
  },
  "highlight": {
    "fields": { "evidence_text": {} }
  }
}
```

##### 4. Neo4j å…³ç³»å›¾è°±ï¼ˆå¯è§†åŒ–äººç‰©ç½‘ç»œï¼‰
```cypher
// åˆ›å»ºäººç‰©èŠ‚ç‚¹
CREATE (daiyu:Character {name: "æ—é»›ç‰", gender: "å¥³"})
CREATE (baoyu:Character {name: "è´¾å®ç‰", gender: "ç”·"})

// åˆ›å»ºå…³ç³»ï¼ˆå¸¦å±æ€§ï¼‰
MATCH (a:Character {name: "æ—é»›ç‰"}), (b:Character {name: "è´¾å®ç‰"})
CREATE (a)-[r:RELATION {
  type: "æƒ…æ„Ÿè¯•æ¢",
  evidence: "ä½ ä½•è‹¦å¦‚æ­¤",
  confidence: 0.85,
  chapter: 23
}]->(b)

// æŸ¥è¯¢ï¼šé»›ç‰ä¸å®ç‰çš„æ‰€æœ‰äº’åŠ¨
MATCH (a:Character {name: "æ—é»›ç‰"})-[r]-(b:Character {name: "è´¾å®ç‰"})
RETURN r.type, r.evidence, r.chapter
ORDER BY r.chapter
```

---

## ğŸš€ å¿«é€Ÿå¯åŠ¨ï¼š5æ­¥éªŒè¯ä½ çš„å°è¯´

### æ­¥éª¤1ï¼šå‡†å¤‡ç¯å¢ƒ
```bash
# å®‰è£…æ ¸å¿ƒåº“
pip install spacy ltp transformers openai
python -m spacy download zh_core_web_lg

# å¯åŠ¨æœ¬åœ°LLMï¼ˆOllamaï¼‰
ollama pull qwen:7b  # æˆ–ä½¿ç”¨ chatglm3:6b
ollama serve  # é»˜è®¤ç›‘å¬ 11434 ç«¯å£
```

### æ­¥éª¤2ï¼šå‡†å¤‡å°è¯´ç‰‡æ®µï¼ˆç¤ºä¾‹ï¼‰
```python
novel_snippet = """
ç¬¬ä¸‰å›ï¼šæ—é»›ç‰åˆè¿›è´¾åºœï¼Œè§å®ç‰æ‘”ç‰ï¼Œå¿ƒä¸­æš—æƒŠã€‚æ™šé—´è¢­äººæ¥æŠ¥ï¼Œè¯´å®ç‰å› å¥¹è€Œæ‘”ç‰ï¼Œé»›ç‰å‚æ³ªè‡³ä¸‰æ›´ã€‚æ¬¡æ—¥å®ç‰é£æ™´é›¯é€æ¥æ—§å¸•ï¼Œé»›ç‰è§å¸•ä¸Šæ³ªç—•å®›ç„¶ï¼Œæ€”å¿¡è‰¯ä¹…ã€‚
"""
```

### æ­¥éª¤3ï¼šè¿è¡ŒæŠ½å–ï¼ˆå®Œæ•´ä»£ç è§é™„å½•ï¼‰
```python
extractor = RelationExtractor()
relations = extractor.extract(novel_snippet, {"æ—é»›ç‰", "è´¾å®ç‰", "è¢­äºº", "æ™´é›¯"})
print(json.dumps(relations, indent=2, ensure_ascii=False))
```

### æ­¥éª¤4ï¼šé¢„æœŸè¾“å‡º
```json
[
  {
    "person_a": "è´¾å®ç‰",
    "person_b": "æ—é»›ç‰",
    "relation": "æƒ…æ„Ÿæ¿€çƒˆï¼ˆæ‘”ç‰ï¼‰",
    "evidence": "å› å¥¹è€Œæ‘”ç‰",
    "confidence": 0.93
  },
  {
    "person_a": "æ—é»›ç‰",
    "person_b": "è´¾å®ç‰",
    "relation": "éšç§˜æ€å¿µ",
    "evidence": "è§å¸•ä¸Šæ³ªç—•å®›ç„¶",
    "confidence": 0.88
  },
  {
    "person_a": "è´¾å®ç‰",
    "person_b": "æ—é»›ç‰",
    "relation": "æš—ä¸­å…³æ€€",
    "evidence": "é£æ™´é›¯é€æ¥æ—§å¸•",
    "confidence": 0.91
  }
]
```

### æ­¥éª¤5ï¼šæœç´¢éªŒè¯
```bash
# æœç´¢"é»›ç‰æ”¶åˆ°ç¤¼ç‰©"
curl -X POST "http://localhost:9200/novel_relations/_search" -H 'Content-Type: application/json' -d'
{
  "query": {
    "bool": {
      "must": [
        { "term": { "person_b.keyword": "æ—é»›ç‰" } },
        { "match": { "relation_type": "ç¤¼ç‰© èµ é€ é€æ¥" } }
      ]
    }
  }
}'
```

---

## âš ï¸ å…³é”®æŒ‘æˆ˜ä¸åº”å¯¹ç­–ç•¥

| æŒ‘æˆ˜                                   | è§£å†³æ–¹æ¡ˆ                                                     |
| -------------------------------------- | ------------------------------------------------------------ |
| **äººç‰©åˆ«å**ï¼ˆ"å®äºŒçˆ·"=å®ç‰ï¼‰          | æ„å»ºäººç‰©åˆ«åå­—å…¸ + ç”¨LLMåšåˆ«åå½’ä¸€åŒ–                         |
| **ä»£è¯çˆ†ç‚¸**ï¼ˆè¿ç»­"ä»–"æŒ‡ä»£ä¸åŒäººï¼‰     | ç”¨LTP/Stanford CoreNLPåšå…±æŒ‡æ¶ˆè§£ + ä¸Šä¸‹æ–‡çª—å£é™åˆ¶            |
| **éšæ™¦è¡¨è¾¾æ¼æ£€**                       | â‘  ç”¨LLMé‡å†™ç‰‡æ®µï¼ˆ"å°†éšæ™¦è¡¨è¾¾è½¬ä¸ºç›´ç™½æè¿°"ï¼‰ â‘¡ ç”¨å¯¹æ¯”å­¦ä¹ è®­ç»ƒä¸“ç”¨REæ¨¡å‹ |
| **é•¿è·ç¦»ä¾èµ–**ï¼ˆç¬¬10ç« äº‹ä»¶å½±å“ç¬¬50ç« ï¼‰ | â‘  æŒ‰ç« èŠ‚åˆ†æ®µæŠ½å– â‘¡ ç”¨å›¾ç®—æ³•èšåˆè·¨ç« èŠ‚å…³ç³»                    |
| **è®¡ç®—æˆæœ¬é«˜**                         | â‘  è§„åˆ™å¼•æ“è¿‡æ»¤80%ç®€å•ç‰‡æ®µ â‘¡ ä»…å¯¹å¤æ‚ç‰‡æ®µè°ƒç”¨LLM â‘¢ ç”¨7Bå°æ¨¡å‹æœ¬åœ°éƒ¨ç½² |

---

## ğŸ“ é™„å½•ï¼šå®Œæ•´å¯è¿è¡Œä»£ç ï¼ˆç®€åŒ–ç‰ˆï¼‰

```python
# relation_extractor.py
import json, re
from openai import OpenAI

class NovelRelationMiner:
    def __init__(self, llm_base_url="http://localhost:11434/v1"):
        self.client = OpenAI(base_url=llm_base_url, api_key="ollama")
        self.model = "qwen:7b"
    
    def extract_from_text(self, text, characters):
        # æ„å»ºç²¾å‡†æç¤ºè¯
        prompt = f"""ä»»åŠ¡ï¼šä»å°è¯´ç‰‡æ®µä¸­æŠ½å–äººç‰©å…³ç³»äº‹ä»¶
äººç‰©åˆ—è¡¨ï¼š{', '.join(characters)}
è¦æ±‚ï¼š
1. ä»…æŠ½å–åˆ—è¡¨ä¸­äººç‰©çš„å…³ç³»
2. å…³ç³»ç±»å‹è‡ªç”±æè¿°ï¼ˆå¦‚"æš—ä¸­ä¿æŠ¤"ã€"è¨€è¯­è¯•æ¢"ï¼‰ï¼Œä¸è¦é™äºé¢„è®¾ç±»å‹
3. è¯æ®ç‰‡æ®µå¿…é¡»æ¥è‡ªåŸæ–‡ï¼Œä¸è¶…è¿‡15å­—
4. è¾“å‡ºçº¯JSONæ•°ç»„ï¼Œæ— å…¶ä»–æ–‡å­—

æ–‡æœ¬ï¼š{text}

è¾“å‡ºæ ¼å¼ï¼š
[{{"person_a": "A", "person_b": "B", "relation": "å…³ç³»æè¿°", "evidence": "åŸæ–‡ç‰‡æ®µ", "confidence": 0.8}}]
"""
        try:
            resp = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=500
            )
            # æ¸…ç†LLMå¯èƒ½æ·»åŠ çš„markdown
            content = resp.choices[0].message.content.strip()
            content = re.sub(r'^```json\n|\n```$', '', content)
            return json.loads(content)
        except Exception as e:
            print(f"LLMè§£æå¤±è´¥: {e}")
            return []
    
    def search_relations(self, es_client, person_a, person_b, relation_keyword=None):
        """åœ¨ESä¸­æœç´¢å…³ç³»"""
        query = {
            "bool": {
                "must": [
                    {"terms": {"person_a.keyword": [person_a, person_b]}},
                    {"terms": {"person_b.keyword": [person_a, person_b]}}
                ],
                "filter": [{"range": {"confidence": {"gte": 0.7}}}]
            }
        }
        if relation_keyword:
            query["bool"]["must"].append(
                {"match": {"relation_type": relation_keyword}}
            )
        return es_client.search(index="novel_relations", body={"query": query, "size": 10})

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    miner = NovelRelationMiner()
    
    text = "é»›ç‰è§å®ç‰æ‘”ç‰ï¼Œå¿ƒä¸­æš—æƒŠã€‚æ™šé—´è¢­äººæ¥è¯´ï¼Œå®ç‰å› å¥¹æ‘”ç‰ï¼Œé»›ç‰å‚æ³ªã€‚æ¬¡æ—¥å®ç‰é£æ™´é›¯é€æ—§å¸•ï¼Œé»›ç‰è§å¸•ä¸Šæ³ªç—•ï¼Œæ€”å¿¡è‰¯ä¹…ã€‚"
    chars = {"æ—é»›ç‰", "è´¾å®ç‰", "è¢­äºº", "æ™´é›¯"}
    
    relations = miner.extract_from_text(text, chars)
    print(json.dumps(relations, indent=2, ensure_ascii=False))
    
    # è¾“å‡ºï¼š
    # [
    #   {"person_a": "è´¾å®ç‰", "person_b": "æ—é»›ç‰", "relation": "æƒ…æ„Ÿæ¿€çƒˆ", "evidence": "å› å¥¹æ‘”ç‰", "confidence": 0.92},
    #   {"person_a": "è´¾å®ç‰", "person_b": "æ—é»›ç‰", "relation": "éšç§˜å…³æ€€", "evidence": "é£æ™´é›¯é€æ—§å¸•", "confidence": 0.89},
    #   {"person_a": "æ—é»›ç‰", "person_b": "è´¾å®ç‰", "relation": "æƒ…æ„Ÿè„†å¼±", "evidence": "å‚æ³ª...æ€”å¿¡", "confidence": 0.85}
    # ]
```

---

## ğŸ’¡ ç»ˆæå»ºè®®ï¼šåˆ†é˜¶æ®µå®æ–½

| é˜¶æ®µ        | ç›®æ ‡         | æŠ€æœ¯æ ˆ               | è€—æ—¶  |
| ----------- | ------------ | -------------------- | ----- |
| **Phase 1** | å•æœ¬å°è¯´éªŒè¯ | è§„åˆ™+æœ¬åœ°LLM         | 2å°æ—¶ |
| **Phase 2** | å¤šæœ¬å°è¯´æ‰©å±• | åŠ å…¥å…±æŒ‡æ¶ˆè§£+ESç´¢å¼•  | 1å¤©   |
| **Phase 3** | äº§å“åŒ–       | Neo4jå›¾è°±+å‰ç«¯å¯è§†åŒ– | 1å‘¨   |

> âœ¨ **å…³é”®æ´å¯Ÿ**ï¼š  
> - **ä¸è¦è¿½æ±‚100%å‡†ç¡®**ï¼šå…ˆè¦†ç›–é«˜é¢‘æ˜¾æ€§å…³ç³»ï¼ˆå‡†ç¡®ç‡>90%ï¼‰ï¼Œå†ç”¨LLMè¡¥æ¼éšæ€§å…³ç³»  
> - **äººå·¥æ ¡éªŒé—­ç¯**ï¼šå°†ä½ç½®ä¿¡åº¦ç»“æœå¯¼å‡ºä¾›äººå·¥å®¡æ ¸ï¼Œåé¦ˆä¼˜åŒ–æç¤ºè¯  
> - **é¢†åŸŸé€‚é…**ï¼šå¤é£å°è¯´éœ€åœ¨æç¤ºè¯ä¸­åŠ å…¥"æ‘”ç‰=æƒ…æ„Ÿæ¿€çƒˆ"ç­‰é¢†åŸŸçŸ¥è¯†  

**è®°ä½**ï¼šå…³ç³»æŠ½å–çš„æœ¬è´¨æ˜¯**è¯­ä¹‰å‹ç¼©**â€”â€”å°†åƒå­—æ–‡æœ¬å‹ç¼©ä¸º"äººç‰©A-å…³ç³»-äººç‰©B"ä¸‰å…ƒç»„ã€‚ç”¨LLMåš"è¯­ä¹‰ç¿»è¯‘å™¨"ï¼Œç”¨ESåš"å…³ç³»æœç´¢å¼•æ“"ï¼Œä½ å°±èƒ½æ„å»ºå°è¯´ä¸–ç•Œçš„æ•°å­—å­ªç”Ÿ ğŸŒŒ